{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "# 01 DATA TYPE\n",
        "\n",
        "In this session, weâ€™ll explore one of the most fundamental concepts in Python programming: Data Types. Understanding data types is essential because everything in Python is an object with a specific type.\n",
        "\n",
        "ðŸ“˜ **What Youâ€™ll Learn: **\n",
        "\n",
        "*What are data types in Python?*\n",
        "    The most common built-in data types\n",
        "\n",
        "    int â€“ Integer numbers\n",
        "\n",
        "    float â€“ Decimal numbers\n",
        "\n",
        "    str â€“ Text data (strings)\n",
        "\n",
        "    bool â€“ Boolean values (True / False)\n",
        "\n",
        "    list, tuple, dict, set â€“ Collection types\n",
        "\n",
        "*How to check a variableâ€™s data type:*\n",
        "\n",
        "    Type conversion (casting)"
      ],
      "metadata": {
        "id": "xngvLSzAzZvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFvHErhpxPY1"
      },
      "outputs": [],
      "source": [
        "#Example of Primitive Data Types - Introduction\n",
        "\n",
        "\n",
        "# Example: Declare different types of variables\n",
        "a = 10          # Integer\n",
        "b = 3.14        # Float\n",
        "c = \"Hello\"     # String\n",
        "d = True        # Boolean\n",
        "\n",
        "# Check their types\n",
        "print(type(a))\n",
        "print(type(b))\n",
        "print(type(c))\n",
        "print(type(d))\n",
        "\n",
        "print (\"\\nData type can operate differently\")\n",
        "print(type(3 + 5.0))\n",
        "print(type(\"3\" + \"5\"))\n",
        "print(type(10 > 3))\n",
        "\n",
        "print (\"\\nisinstance to call Boolean\")\n",
        "print(isinstance(3,float))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 01\n",
        "# Goal        : Practice basic operations\n",
        "# Description :\n",
        "# - Practice using Gemini in Google Colab\n",
        "# - You have list that has item [1, '2', \"Finance\", \"4\", \"News\", 10]\n",
        "#.  What is the logic to obtain result : Print out total value of numbers - both integer and string - in the list\n",
        "# - Duration : 5mins\n",
        "\n",
        "data_list = [1, '2', \"Finance\", \"4\", \"News\", 10]\n",
        "total_value = 0\n",
        "\n",
        "for item in data_list:\n",
        "  if isinstance(item, (int, float)):\n",
        "    total_value += item\n",
        "  elif isinstance(item, str) and item.isdigit():\n",
        "    total_value += int(item)\n",
        "\n",
        "print(total_value)\n"
      ],
      "metadata": {
        "id": "yatBdIW-kYY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introducing another data type, popular for data analysis : Data Frame**\n",
        "\n",
        "\n",
        "\n",
        "*   Provided by the Pandas library in Python, not a built-in native Python type.\n",
        "*   A class used to represent tabular data (think of it like an excel sheet)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GQmfOp2rjA0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Blank dataframe from pandas (1)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Assign variables to dataframe\n",
        "df = pd.DataFrame(columns=[\"newsportal\", \"title\", \"date\"])\n",
        "\n",
        "newsportal = \"Detik\"\n",
        "title = \"BI Ungkap Strategi Menjaga Inflasi\"\n",
        "date = \"2025-05-31\"\n",
        "\n",
        "\n",
        "df.loc[len(df)] = [newsportal, title, date]\n",
        "\n",
        "# View the DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "waWZ4aOQpzqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframe from pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with news portal data\n",
        "df = pd.DataFrame({\n",
        "    'newsportal': ['Detik', 'Kompas'],\n",
        "    'title': [\n",
        "        'BI Ungkap Strategi Menjaga Inflasi',\n",
        "        'Kerangka Ekonomi Makro 2026: Pertumbuhan Ekonomi 5,2 hingga 5,8 Persen'\n",
        "    ],\n",
        "    'date': ['2025-05-22', '2025-06-01']\n",
        "})\n",
        "\n",
        "# Print the type of the DataFrame\n",
        "print(type(df))"
      ],
      "metadata": {
        "id": "2ckVjcXhj0v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KyvKc89vkicy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV in the Colab working directory\n",
        "df.to_csv(\"news_articles.csv\", index=False)\n",
        "\n",
        "# Save to Excel (requires openpyxl for .xlsx)\n",
        "df.to_excel(\"news_articles.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "maIO5eETlOnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 02\n",
        "# Goal        : Practice loading csv / excel to Google Colab\n",
        "# Description :\n",
        "# - Create an excel sheet consists of : header row, 3 columns, 3 rows\n",
        "# - Load to Colab Directory (on the left side, folder icon)\n",
        "# - Practice Gemini to get syntax of loading to here\n",
        "# - Duration : 10mins\n",
        "\n",
        "# [Your script here]\n"
      ],
      "metadata": {
        "id": "AQ71-aBZmmNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 String Operation\n",
        "\n",
        "ðŸ“Œ If you canâ€™t handle strings, you canâ€™t clean scraped data.\n",
        "\n",
        "String operations are your basic toolbox for transforming raw scraped content into clean, usable data â€” whether youâ€™re building a dataset, summarizing news, or tracking trends."
      ],
      "metadata": {
        "id": "lE_o2aIMBUgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 03\n",
        "# Goal        : Execute String Operations as foundations to text (scraping) analysis\n",
        "# Description :\n",
        "# Practice using Gemini to get the syntax for below use cases:\n",
        "# Clean the headline - Remove leading/trailing whitespace, then Convert to lowercase\n",
        "# Expected print result : \"bi ungkap strategi menjaga inflasi\"\n",
        "\n",
        "# Extract the authorâ€™s name and date from author_info\n",
        "# Expected print result \"Retno Ayuningrum\"\n",
        "\n",
        "# Remove the link from the article snippet\n",
        "# Expected print result \"22 Mei 2025\"\n",
        "\n",
        "# Keep only the sentence, not the URL\n",
        "# Expected print result : \" Bank Indonesia (BI) menyampaikan bahwa meskipun inflasi global diperkirakan meningkat pada 2025, inflasi di Indonesia tetap stabil dan berada dalam sasaran yang ditetapkan, berkat sinergi antara BI dan pemerintah melalui Tim Pengendalian Inflasi Pusat dan Daerah (TPIP-TPID) dalam menjaga stabilitas harga pangan.\"\n",
        "\n",
        "\n",
        "headline = \"   BI Ungkap Strategi Menjaga Inflasi   \"\n",
        "author_info = \"By Retno Ayuningrum - 22 Mei 2025\"\n",
        "article_snippet = \"Bank Indonesia (BI) menyampaikan bahwa meskipun inflasi global \\\n",
        "diperkirakan meningkat pada 2025, inflasi di Indonesia tetap stabil dan berada dalam sasaran yang ditetapkan, \\\n",
        "berkat sinergi antara BI dan pemerintah melalui Tim Pengendalian Inflasi Pusat dan Daerah (TPIP-TPID) \\\n",
        "dalam menjaga stabilitas harga pangan. \\\n",
        " Read more at https://finance.detik.com/berita-ekonomi-bisnis/d-7926539/bi-ungkap-strategi-menjaga-inflasi\"\n",
        "\n",
        "# [Your script here]\n",
        "\n",
        "# Clean the headline - Remove leading/trailing whitespace, then Convert to lowercase\n",
        "cleaned_headline = headline.strip().lower()\n",
        "print(cleaned_headline)\n",
        "\n",
        "# Extract the authorâ€™s name and date from author_info\n",
        "# Split the string by ' - ' and take the second element, then remove 'By '\n",
        "author_name = author_info.split(' - ')[0].replace('By ', '')\n",
        "print(author_name)\n",
        "\n",
        "# Remove the link from the article snippet\n",
        "# Find the index of 'Read more at' and slice the string\n",
        "date_info = author_info.split(' - ')[1]\n",
        "print(date_info)\n",
        "\n",
        "\n",
        "# Keep only the sentence, not the URL\n",
        "# Find the index of 'Read more at' and slice the string before that\n",
        "url_start_index = article_snippet.find(\" Read more at\")\n",
        "if url_start_index != -1:\n",
        "  article_sentence = article_snippet[:url_start_index].strip()\n",
        "else:\n",
        "  article_sentence = article_snippet.strip()\n",
        "\n",
        "print(article_sentence)\n"
      ],
      "metadata": {
        "id": "HjD7xdD8CD72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is sample of news scraping block of syntax of single news portal using BeautifulSoup\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the Detik Finance article\n",
        "url = \"https://finance.detik.com/berita-ekonomi-bisnis/d-7926539/bi-ungkap-strategi-menjaga-inflasi\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract the title\n",
        "title_tag = soup.find(\"h1\")\n",
        "title = title_tag.get_text(strip=True) if title_tag else \"Title not found\"\n",
        "\n",
        "# Extract the author and date\n",
        "date_tag = soup.find(\"div\", class_=\"detail__date\")\n",
        "\n",
        "# Extract the author from the meta tag\n",
        "author_meta = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
        "author = author_meta[\"content\"] if author_meta else \"Author not found\"\n",
        "date = date_tag.get_text(strip=True) if date_tag else \"Date not found\"\n",
        "\n",
        "# Extract the article content\n",
        "content_div = soup.find(\"div\", class_=\"detail__body-text\")\n",
        "if content_div:\n",
        "    paragraphs = content_div.find_all(\"p\")\n",
        "    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
        "else:\n",
        "    content = \"Content not found\"\n",
        "\n",
        "# Display the extracted information\n",
        "print(\"Title:\", title)\n",
        "print(\"Author:\", author)\n",
        "print(\"Date:\", date)\n",
        "print(\"Content:\\n\", content)\n"
      ],
      "metadata": {
        "id": "23Scw5qLxanS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 04\n",
        "# Goal        : Execute String Operations (real example)\n",
        "# Description :\n",
        "# Practice using Gemini to get the syntax for below use cases:\n",
        "# Extract the date from date variable \"Kamis, 22 Mei  2025 12:43 WIB.\"\n",
        "# Duration : 10mins\n",
        "\n",
        "date=\"Kamis, 22 Mei  2025 12:43 WIB.\"\n",
        "\n",
        "# [Your script here]\n",
        "\n",
        "\n",
        "# Split the string by comma and take the second part\n",
        "date_part = date.split(',')[1].strip()\n",
        "\n",
        "# Split the date part by space and take the first three parts (day, month, year)\n",
        "date_components = date_part.split()[:3]\n",
        "\n",
        "# Join the components back with a space\n",
        "extracted_date = ' '.join(date_components)\n",
        "\n",
        "print(extracted_date)"
      ],
      "metadata": {
        "id": "fmVIvcG_xiQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 05\n",
        "# Goal        : Create dataframe that contains Title, Author, Date, and URL\n",
        "# Description :\n",
        "# Create a variable detik_data which is the dictionary\n",
        "# Create dataframe called df_news that contains detiknews component of title, author, date, url (4 columns)\n",
        "# Duration : 10mins\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# [Your script here]\n",
        "\n",
        "detik_data = {\n",
        "    'Title': [title],\n",
        "    'Author': [author],\n",
        "    'Date': [date],\n",
        "    'URL': [url]\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "df_news = pd.DataFrame(detik_data)\n",
        "df_news\n"
      ],
      "metadata": {
        "id": "QrEZZw9HQ27c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03 Scrape a single news page"
      ],
      "metadata": {
        "id": "2sa4mU_0OTV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautiful Soup scrapes and analyzes web page content like article titles, links, dates, and more â€” by treating the HTML as a tree of elements that you can search."
      ],
      "metadata": {
        "id": "j07-LrJjOX1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = \"\"\"\n",
        "<html>\n",
        "  <body>\n",
        "    <h1>Hello, world!</h1>\n",
        "    <p class=\"intro\">Welcome to scraping</p>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Extract elements\n",
        "print(soup.h1.text)                # Hello, world!\n",
        "print(soup.find(\"p\").text)         # Welcome to scraping\n"
      ],
      "metadata": {
        "id": "XUU7oYcjr7m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 06\n",
        "# Goal        : Newscraping practice using BeautifulSoup\n",
        "# Description :\n",
        "# Using scraping syntax provided previously, newscrape an article from Kompas\n",
        "# [Attention] Use URL = https://money.kompas.com/read/2025/05/20/145400226/kerangka-ekonomi-makro-2026--pertumbuhan-ekonomi-5-2-hingga-5-8-persen\n",
        "# Why Date and Content scraping not found? [Hint] Inspect and find that content is under <script>\n",
        "# Duration : 15mins\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "\n",
        "# URL of the Detik Finance article\n",
        "url = \"https://money.kompas.com/read/2025/05/20/145400226/kerangka-ekonomi-makro-2026--pertumbuhan-ekonomi-5-2-hingga-5-8-persen\"\n",
        "\n",
        "# [Your script here]\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract the title\n",
        "title_tag = soup.find(\"h1\")\n",
        "title = title_tag.get_text(strip=True) if title_tag else \"Title not found\"\n",
        "\n",
        "# Extract the author and date\n",
        "date_tag = soup.find(\"div\", class_=\"read__time\")\n",
        "\n",
        "# Extract the author from the meta tag\n",
        "author_meta = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
        "author = author_meta[\"content\"] if author_meta else \"Author not found\"\n",
        "date = date_tag.get_text(strip=True) if date_tag else \"Date not found\"\n",
        "\n",
        "# Find all script tags\n",
        "script_tags = soup.find_all(\"script\")\n",
        "\n",
        "# Iterate through script tags and search for the variable\n",
        "for script in script_tags:\n",
        "    if script.string and \"var keywordBrandSafety\" in script.string:\n",
        "        # Use regex to find the value of keywordBrandSafety\n",
        "        match = re.search(r'var keywordBrandSafety\\s*=\\s*\"(.*?)\";', script.string)\n",
        "        if match:\n",
        "            # Extract the captured group (the content within the quotes)\n",
        "            content = match.group(1)\n",
        "            break  # Stop searching once found\n",
        "\n",
        "# Display the extracted information\n",
        "print(\"Title:\", title)\n",
        "print(\"Author:\", author)\n",
        "print(\"Date:\", date)\n",
        "print(\"Content:\\n\", content)"
      ],
      "metadata": {
        "id": "PJZsfUwmPPjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 07\n",
        "# Goal        : Append to dataframe (do not create new dataframe)\n",
        "# Description :\n",
        "# Append kompas component from result above (Title, Author, Date, and URL) to df_news (previously created).\n",
        "# Expected result : df_news now has 4 columns and 2 rows\n",
        "# Duration : 10mins\n",
        "\n",
        "# [Your script here]\n",
        "\n",
        "# Create a dictionary for the Kompas article data\n",
        "kompas_data = {\n",
        "    'Title': [title],\n",
        "    'Author': [author],\n",
        "    'Date': [date],\n",
        "    'URL': [url]\n",
        "}\n",
        "\n",
        "# Create a new DataFrame from the Kompas data\n",
        "df_kompas = pd.DataFrame(kompas_data)\n",
        "\n",
        "# Append the Kompas DataFrame to the existing df_news DataFrame\n",
        "# Use ignore_index=True to renumber the index after appending\n",
        "df_news = pd.concat([df_news, df_kompas], ignore_index=True)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_news\n",
        "\n"
      ],
      "metadata": {
        "id": "lQoY1YICsFlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 Scrape a news portal with a keyword"
      ],
      "metadata": {
        "id": "Mo5_VTNcuk4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Step 1: Define search URL\n",
        "search_url = \"https://www.detik.com/search/searchall?query=inflasi&siteid=29&source_kanal=true\"\n",
        "\n",
        "# Step 2: Fetch search result page\n",
        "response = requests.get(search_url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Step 3: Extract top 5 article URLs\n",
        "article_links = []\n",
        "for tag in soup.select(\"article a[href]\"):\n",
        "    url = tag[\"href\"]\n",
        "    if url.startswith(\"https://\") and url not in article_links:\n",
        "        article_links.append(url)\n",
        "    if len(article_links) == 5:\n",
        "        break\n",
        "\n",
        "print(f\"\\nFound {len(article_links)} article links.\\n\")\n",
        "\n",
        "# Step 4: Prepare a list to collect all articles\n",
        "rows = []\n",
        "\n",
        "# Step 5: Scrape each article\n",
        "for link in article_links:\n",
        "    try:\n",
        "        article_resp = requests.get(link)\n",
        "        article_soup = BeautifulSoup(article_resp.text, \"html.parser\")\n",
        "\n",
        "        title = article_soup.find(\"h1\").get_text(strip=True) if article_soup.find(\"h1\") else \"No Title\"\n",
        "        author_meta = article_soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
        "        author = author_meta[\"content\"] if author_meta else \"No Author\"\n",
        "        date_tag = article_soup.find(\"div\", class_=\"detail__date\")\n",
        "        date = date_tag.get_text(strip=True) if date_tag else \"No Date\"\n",
        "\n",
        "        # Print info\n",
        "        print(\"Title :\", title)\n",
        "        print(\"Author:\", author)\n",
        "        print(\"Date  :\", date)\n",
        "        print(\"URL   :\", link)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Append to list\n",
        "        rows.append({\n",
        "            \"title\": title,\n",
        "            \"author\": author,\n",
        "            \"date\": date,\n",
        "            \"url\": link\n",
        "        })\n",
        "\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {link}: {e}\")\n",
        "\n",
        "# Step 6: Convert to DataFrame\n",
        "df_multinews= pd.DataFrame(rows)\n",
        "df_multinews\n"
      ],
      "metadata": {
        "id": "9s7aQhQguxIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 08\n",
        "# Goal        : Newscraping from a single news portal using multi keyword\n",
        "# Description :\n",
        "# Create a list of keyword keyword_list=['inflasi','perang dagang'] from finance detik news\n",
        "# Create function to contain newscraping script but with top 3 per keyword\n",
        "# Create loop to read every item in the list and pass it to the function\n",
        "# Duration : 10mins\n",
        "\n",
        "keyword_list=['inflasi','perang dagang']\n",
        "\n",
        "def extract_multikeyword(keyword):\n",
        "\n",
        "  # Step 1: Define search URL\n",
        "  search_url = f\"https://www.detik.com/search/searchall?query={keyword}&siteid=29&source_kanal=true\"\n",
        "\n",
        "  # Step 2: Fetch search result page\n",
        "  response = requests.get(search_url)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  # Step 3: Extract top 5 article URLs\n",
        "  article_links = []\n",
        "  for tag in soup.select(\"article a[href]\"):\n",
        "      url = tag[\"href\"]\n",
        "      if url.startswith(\"https://\") and url not in article_links:\n",
        "          article_links.append(url)\n",
        "      if len(article_links) == 3:\n",
        "          break\n",
        "\n",
        "  print(f\"\\nFound {len(article_links)} article links.\\n\")\n",
        "\n",
        "  # Step 4: Prepare a list to collect all articles\n",
        "  rows = []\n",
        "\n",
        "  # Step 5: Scrape each article\n",
        "  for link in article_links:\n",
        "      try:\n",
        "          article_resp = requests.get(link)\n",
        "          article_soup = BeautifulSoup(article_resp.text, \"html.parser\")\n",
        "\n",
        "          title = article_soup.find(\"h1\").get_text(strip=True) if article_soup.find(\"h1\") else \"No Title\"\n",
        "          author_meta = article_soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
        "          author = author_meta[\"content\"] if author_meta else \"No Author\"\n",
        "          date_tag = article_soup.find(\"div\", class_=\"detail__date\")\n",
        "          date = date_tag.get_text(strip=True) if date_tag else \"No Date\"\n",
        "\n",
        "          # Print info\n",
        "          print(\"Title :\", title)\n",
        "          print(\"Author:\", author)\n",
        "          print(\"Date  :\", date)\n",
        "          print(\"URL   :\", link)\n",
        "          print(\"-\" * 60)\n",
        "\n",
        "          # Append to list\n",
        "          rows.append({\n",
        "              \"title\": title,\n",
        "              \"author\": author,\n",
        "              \"date\": date,\n",
        "              \"url\": link\n",
        "          })\n",
        "\n",
        "          time.sleep(1)\n",
        "      except Exception as e:\n",
        "          print(f\"Error scraping {link}: {e}\")\n",
        "\n",
        "  return rows\n",
        "\n",
        "rows_all=[]\n",
        "for keyword in keyword_list:\n",
        "  result=extract_multikeyword(keyword)\n",
        "  rows_all.extend (result)\n",
        "\n",
        "df_multinewskeyword= pd.DataFrame(rows_all)\n",
        "df_multinewskeyword"
      ],
      "metadata": {
        "id": "CAnyMtL7xuVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 09\n",
        "# Goal        : Sort df_multinewskeyword by date\n",
        "# Description :\n",
        "# [Hint] Date is different format, should be splitted.\n",
        "# [Prompt Example] How to split the df_multinewskeyword[date] to extract\n",
        "#                  from Rabu, 14 Mei 2025 11:12 WIB to 14 Mei 2025 only, then convert to date type.\n",
        "#                  The text string is month Mei, Apr, Nov, Okt -- need mapping\n",
        "# Duration : 15mins\n",
        "\n",
        "# [Your script here]\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Assuming df_multinewskeyword DataFrame already exists with the 'date' column\n",
        "\n",
        "# Manual mapping for 3-character Indonesian month abbreviations to English full names\n",
        "month_abbr_to_english = {\n",
        "    'Jan': 'January',\n",
        "    'Feb': 'February',\n",
        "    'Mar': 'March',\n",
        "    'Apr': 'April',\n",
        "    'Mei': 'May',\n",
        "    'Jun': 'June',\n",
        "    'Jul': 'July',\n",
        "    'Agu': 'August',\n",
        "    'Sep': 'September',\n",
        "    'Okt': 'October',\n",
        "    'Nov': 'November',\n",
        "    'Des': 'December'\n",
        "}\n",
        "\n",
        "\n",
        "# Function to extract, map month, and convert to date\n",
        "def extract_map_and_convert_date(date_str):\n",
        "    if date_str == \"No Date\":\n",
        "        return pd.NaT # Return Not a Time for missing dates\n",
        "\n",
        "    try:\n",
        "        # Split by comma and take the second part (which contains date and time)\n",
        "        date_time_part = date_str.split(',')[1].strip()\n",
        "\n",
        "        # Split the date_time_part by space\n",
        "        parts = date_time_part.split()\n",
        "\n",
        "        if len(parts) < 3: # Ensure we have at least day, month, year\n",
        "             return pd.NaT\n",
        "\n",
        "        # Extract day, month abbreviation, and year\n",
        "        day = parts[0]\n",
        "        month_abbr_id = parts[1]\n",
        "        year = parts[2]\n",
        "\n",
        "        # Map Indonesian month abbreviation to English full name\n",
        "        month_en = month_abbr_to_english.get(month_abbr_id, month_abbr_id) # Use original if not found\n",
        "\n",
        "        # Construct a string in a format that pd.to_datetime can easily understand\n",
        "        # using the full English month name\n",
        "        cleaned_date_str = f\"{day} {month_en} {year}\"\n",
        "\n",
        "        # Convert to datetime using pd.to_datetime\n",
        "        # Use the format string that matches \"14 May 2025\"\n",
        "        return pd.to_datetime(cleaned_date_str, format='%d %B %Y', errors='coerce')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing date '{date_str}': {e}\")\n",
        "        return pd.NaT # Return Not a Time if conversion fails\n",
        "\n",
        "\n",
        "# Apply the function to the 'date' column and store in a new column\n",
        "df_multinewskeyword['date_converted_mapped'] = df_multinewskeyword['date'].apply(extract_map_and_convert_date)\n",
        "\n",
        "# Display the DataFrame with the new converted date column\n",
        "print(df_multinewskeyword[['title', 'date', 'date_converted_mapped']])\n",
        "\n",
        "# Now you can sort by the 'date_converted_mapped' column\n",
        "df_multinewskeyword_sorted = df_multinewskeyword.sort_values(by='date_converted_mapped', ascending=True)\n",
        "\n",
        "df_multinewskeyword_sorted\n"
      ],
      "metadata": {
        "id": "gh-PiEOpPbIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 10\n",
        "# Goal        : Save final result to local desktop\n",
        "# Description : Save df_multinewskeyword_sorted to \"\"detik_finance_news_sorted.xlsx\"\" and download to local desktop\n",
        "\n",
        "\n",
        "# [Your script here]\n",
        "\n",
        "# Define the filename for the Excel file\n",
        "excel_filename = \"detik_finance_news_sorted.xlsx\"\n",
        "\n",
        "# Save the DataFrame to an Excel file in the Colab working directory\n",
        "# index=False prevents writing the DataFrame index as a column in the Excel file\n",
        "df_multinewskeyword_sorted.to_excel(excel_filename, index=False)\n"
      ],
      "metadata": {
        "id": "Ha074j1wcMjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}